<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Building a Convolutional Neural Network for NLP</title>
  <meta name="description" content="  ">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/global.css">

  <!-- JS -->
  <script type="text/javascript" src="/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  <!-- script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script -->

  <link rel="canonical" href="http://xstatic.chewning.co/neural_network/2016/07/25/CNN_NLP.html">
  <link rel="alternate" type="application/rss+xml" title="chewning.co" href="http://xstatic.chewning.co/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <!-- a class="site-title" href="/">chewning.co</a -->

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
          <a class="page-link" href="/index.html">&raquo; chewning.co</a> |
        
          
          <a class="page-link" href="/about/">about</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Building a Convolutional Neural Network for NLP</h1>
    <p class="post-meta"><time datetime="2016-07-25T00:28:57-04:00" itemprop="datePublished">07.25.16</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <div align="center">
  <img src="/images/writing-med.jpg" />
</div>

<h3 id="overview">Overview</h3>
<p>When looking at neural network architecture designs and implementations it can sometimes be hard to make sense
of their monolithic structure and internal workings. When building your own networks or deciphering existing networks 
it is best to test your assumptions
along the way and to build your network layer by layer. Do not worry about performance of the network until
your implementation matches your expectations. It is very easy to get confused by dimensions,
hyper-parameters, and input/output shapes. To this end, this post will focus on building a network similar in spirit to <a href="#references">[1]</a>. 
Here we will decouple the training and performance of a network with the process of <em>building</em> the network. 
Before encapsulating your network in nicely defined functions we can inspect each layer individually and build the
network incrementally, simplifying the process by breaking the process down into manageable parts.
We will build the network using <a href="#references">lasagne</a> and test our expectations at each layer.</p>

<p>The <a href="https://github.com/init-random/neural_nlp/tree/master/sentence_classification/test">associated code</a> and network implementation can be found on github.</p>

<h3 id="the-network">The Network</h3>
<p>We will be building a multi-layer convolutional network for natural language processing classification.
The input into this
model is sentences. Sentences may be of arbitrary length. The convolutional model accommodates for this and extracts
constant length features for a given sentence. Raw text sentences may not be given as input into the network. Instead
we index the vocabulary and each word in the sentence is represented by its index.</p>

<p>The input data into a network is done in batches, which is essentially a list of input arrays. The batch size at each layer
will not change. So, without loss of generality if the batch size is not mentioned you may assume that this is the
first dimension of a layer input/output shape. We now describe the layers of the network that we want to build. Further
detail of each layer will be given below.</p>

<p><strong>Input Layer:</strong> The input into the first layer of the network is an array of indices. The array will be the size
of the sentence and the index for each word is that of the vocabulary as described below.</p>

<p><strong>Embedding Layer:</strong> There is an embedding word index lookup table. Each index in the array of the input layer
is mapped to a vector, which provides a numeric representation for each word, i.e. it’s embedding.</p>

<p><strong>Reshape Layer:</strong> Next we transform the data for the correct shape needed for the convolutional layer, a 4D tensor. 
Once dimension is for the number of channels. The last two dimensions reshapes the embedding into a concatenated
vector. So each sentence will have “height” 1 and “width” sentence length times embedding size. The height and width
terminology come from working with images which was one of the first applications of the convolutional model.</p>

<p><strong>Convolution Layer:</strong> This layer has a number of parameters we need to define. The number of filters chosen effects
the performance of the model, so this is currently not an important parameter. The padding is important here because
we need to pick values that will produce the same output as the sentence length. In essence, this layer is a feature
generator per filter per word window (see below) over the sentence.
See the <a href="#references">references</a> below for more detail on particulars of convolutions and max pooling.</p>

<p><strong>Max Pooling Layer:</strong> Max pooling is reduction technique. For NLP tasks max pooling over time is commonly used.
The time component here refers to word ordering. Later words in the sentence happen later in time. This layer
condenses the sentence features created in the last layer into one feature. This provides a sentence-level feature.</p>

<p><strong>Flatten Layer:</strong> This is basically another reshape layer to concatenate each sentence-level feature per filter.</p>

<p><strong>Output Layer:</strong> The output of this layer is the softmax classification. Here we just create a model with 
a binary classification.</p>

<h3 id="building-the-network">Building the Network</h3>

<p>Let’s define some variables we will need for the network.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">lasagne</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;this is certainly the best tasting coffee ever&#39;</span><span class="p">,</span> 
             <span class="s">&#39;that diner coffee tasted like acid&#39;</span><span class="p">,</span> 
             <span class="s">&#39;what is the difference between coffee and espresso&#39;</span><span class="p">]</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="n">pad</span> <span class="o">=</span> <span class="s">&#39;PAD&#39;</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">pad</span><span class="p">])</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="n">vocab_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="c"># {&#39;PAD&#39;: 16, &#39;acid&#39;: 8, &#39;and&#39;: 3, &#39;best&#39;: 12, &#39;between&#39;: 1, &#39;certainly&#39;: 18, ...</span>

<span class="n">pad_val</span> <span class="o">=</span> <span class="n">vocab_map</span><span class="p">[</span><span class="n">pad</span><span class="p">]</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">sentence_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">])</span>

<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad_val</span><span class="p">]</span> <span class="o">*</span> <span class="n">sentence_length</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab_map</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
    <span class="n">sentence</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">idx</span>
    <span class="n">x_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;int32&#39;</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">imatrix</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;int32&#39;</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_map</span><span class="p">)</span>
<span class="n">sentence_length</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">channels</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sentence_height</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_filters</span> <span class="o">=</span> <span class="mi">31</span>
<span class="n">word_window</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">sentence_height</span> <span class="o">=</span> <span class="mi">1</span></code></pre></div>

<p>The <code>sentence</code> and <code>vocab_map</code> is an example of how you could build a vocabulary index from text. The 
<code>embeddings</code> matrix is the map from the word 
index to its vector representation (embedding). Here we use a random initialization.
See <a href="#references">[1]</a> for details. <code>batch_size</code> is the number of sentences we will use for each training batch; <code>None</code> just
means that it may be of arbitrary length. <code>channels</code> is set to 1 here. If this were a convolutional network for images
<code>channels</code> would be 3 and would represent RGB colors of the image. Text is single dimensional so we will keep this as
1. There are text-related extensions that are possible with channels, but we will not consider those here. Rather we
can keep this as a placeholder for future extensions. This also means that we will need to lasagne’s Conv2DLayer
instead of Conv1DLayer. The latter does not accommodate for channels. Sentences may be of arbitrary length. For
a given batch, however, all sentences must be of the same length. As we will see, all sentences will be of length
<code>sentence_length</code> that we provide as input. Shorter sentences will be right-padded to the appropriate length 
with a special word, like <code>PAD</code>, which will have its own embedding value in the embedding matrix. Below
is what a single raw batch of size 3 would look like. This tensor would be dimension <code>(3, sentence_height, sentence_length)</code>.
The input tensor for the network will be of the same dimension but each word would be replaced by its index
in the vocabulary (as in <code>x_test</code>).</p>

<pre>
&lt;W1&gt;  &lt;W2&gt;                                      ...    &lt;Wn&gt; # words 1..n
this    is certainly        the    best tasting ... PAD PAD # pad to sentence_length
that diner    coffee     tasted    like    acid ... PAD PAD
what    is       the difference between  coffee ... PAD PAD
</pre>

<p><code>num_filters</code> will be the number of filters we use in the convolution. <code>word_window</code> is 3 which is the length of the sliding
window of trigrams as shown below.</p>

<pre>
        |----------------|
     |--------------|
|---------------|
this is certainly the best tasting ...
</pre>

<p>When building a network it is a good idea to use distinct numbers (prime numbers work well). If I defined a number of 
parameters of value 2 and a particular dimension of an output layer had this value, it would be hard to know which variable 
the 2 was representing. The parameters we use here are not the ones we will use when we actually train the model.
We are currently just concerned about the architecture of the network, not its performance. It is also a good idea
not use “magic” numbers in your code. It is best to have clear variable names for all parameters and not hard-coded
values, which makes things very hard to debug.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="err">@</span> <span class="n">b</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>    


<span class="k">def</span> <span class="nf">network_prediction</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">input_var</span><span class="p">,</span> <span class="n">test_data</span><span class="p">):</span>
    <span class="n">layer_prediction</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">layer_predict_function</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">input_var</span><span class="p">],</span> <span class="n">layer_prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_predict_function</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span></code></pre></div>

<p>These two functions are to calculate a convolution and make a prediction from a given input respectively. We will use these
for testing. This is an untrained network, a prediction on this network has no predictive power because we are 
just using the initial weights, but we can still test architecture assumptions of the network.  Let’s go through 
each layer: <code>l_in</code> is the input layer which takes in the sentence indexes. So
we can test the network we need to define a few more variables. <code>x</code> is a symbolic container for the input data. 
<code>x_test</code> is a batch of sample sentences, each value is an index in the vocabulary. Each sentence is right-padded
to size <code>sentence_length</code>. Note the use of <code>astype</code> in the network, <a href="#references">Theano</a> is particular about the type of variables.</p>

<p>We test our input assumptions.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">l_in</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">),</span> <span class="n">input_var</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="c"># output shape: (3, 9)</span>
    
<span class="k">assert</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">)</span> <span class="c"># 3 is the batch_size</span>
<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">l_in</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">)</span></code></pre></div>

<p>The input <code>x_test</code> and the output from <code>l_in</code> should be the same.</p>

<p>The next two layers do the embedding lookup and a reshape. The reshape is to put the data into the proper dimensions
for the convolution. We need the 4D tensor <code>(batch_size, channels, image_height, image_width)</code>. Two-dimensional convolutions
are typically used for images. In our case <code>(image_height, image_width)</code> is equivalent to <code>(sentence_height, sentence_length * embedding_size)</code>.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">embedding</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">l_in</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
                               <span class="n">output_size</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>
<span class="c"># output shape: (3, 9, 3)</span>

<span class="n">reshape</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReshapeLayer</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">channels</span><span class="p">,</span> <span class="n">sentence_height</span><span class="p">,</span> 
                                          <span class="n">embedding</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">embedding</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="c"># output shape: (3, 1, 1, 27)</span>

<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">reshape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">sentence_height</span><span class="p">,</span> <span class="n">sentence_length</span> <span class="o">*</span> <span class="n">embedding_size</span><span class="p">)</span></code></pre></div>

<p>The next two layers are the convolution and pooling layers. Notice that the convolution squashes the <code>channels</code> but 
the output is a function of number of filters. The last dimension is <code>sentence_length</code>. The
convolution test looks at one convolution for one filter. Because the network is untrained we can
ignore the bias, which is zero. The pooling layer extracts
a constant length feature per filter. This is how we can reduce variable length sentences into constant length
feature. The pooling used for NLP is typically max pooling over time <a href="#references">[2]</a>. The last line tests
the correct value for the max pooling for a single feature. The last dimension of the pooling is 1, which means
we extract 1 feature per sentence per filter.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">conv</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2DLayer</span><span class="p">(</span>
            <span class="n">reshape</span><span class="p">,</span> <span class="n">num_filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">filter_size</span><span class="o">=</span><span class="p">(</span><span class="n">sentence_height</span><span class="p">,</span> <span class="n">word_window</span> <span class="o">*</span> <span class="n">embedding_size</span><span class="p">),</span> 
            <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">),</span>
            <span class="n">nonlinearity</span><span class="o">=</span><span class="n">lasagne</span><span class="o">.</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">lasagne</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">GlorotUniform</span><span class="p">(),</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">))</span>
<span class="c"># output shape: (3, 31, 1, 9)</span>

<span class="n">pool</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2DLayer</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> 
                                     <span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_size</span> <span class="o">*</span> <span class="n">word_window</span><span class="p">),</span> 
                                     <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">ignore_border</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c"># output shape: (3, 31, 1, 1)</span>

<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">sentence_height</span><span class="p">,</span> <span class="n">sentence_length</span><span class="p">)</span>
<span class="c"># last dimension 1: max pooling over time extracts a max over the sentence</span>
<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">pool</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">,</span> <span class="n">sentence_height</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c"># test calculation of convolution; note we are only using a linear activation function for now</span>
<span class="k">assert</span> <span class="n">convolution</span><span class="p">(</span><span class="n">network_prediction</span><span class="p">(</span><span class="n">reshape</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">9</span><span class="p">],</span> <span class="n">conv</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">eval</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="c"># test max pooling over time calculation</span>
<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">pool</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">network_prediction</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span></code></pre></div>

<p>The last two layers flatten out the pooling layer which then feeds into the final two-class softmax layer.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">flatten</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">FlattenLayer</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
<span class="c"># output shape: (3, 31)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">lasagne</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseLayer</span><span class="p">(</span><span class="n">flatten</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">lasagne</span><span class="o">.</span><span class="n">nonlinearities</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
<span class="c"># output shape: (3, 2)</span>

<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">flatten</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">network_prediction</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span></code></pre></div>

<p>You should make sure to look at the creation of the convolution and pooling in the network. The padding
and filter size is very important. Changing the padding will change the output shape of the convolution. See
<a href="#references">[4]</a> for a clear explanation.</p>

<h3 id="conclusion">Conclusion</h3>
<p>We built a convolutional neural network for text classification. We were not concerned about the performance
of this network, but rather we tested our expectations at each layer. By not focusing on the learning phase of 
the network this freed us to use unique variable values, which allowed us to separate architecture of the network
from its performance as well as to help facilitate understanding of its operations and dimensions.
By using clear variable names and an avoidance of “magic” (hard-coded) numbers we were able to test our expectations
with clear variable names instead of confusing hard-coded values.</p>

<h3 id="references-a-namereferencesa">References <a name="references"></a></h3>

<ul>
  <li><a href="http://arxiv.org/abs/1408.5882">[1] Convolutional Neural Networks for Sentence Classification, (Kim)</a></li>
  <li><a href="http://jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">[2] Natural Language Processing (Almost) from Scratch, (Collobert, et al.)</a></li>
  <li><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">[3] Understanding Convolutional Neural Networks for NLP</a></li>
  <li><a href="https://github.com/UKPLab/deeplearning4nlp-tutorial/tree/master/2015-10_Lecture/Lecture5">[4] Deep Learning for NLP</a></li>
  <li><a href="http://lasagne.readthedocs.io/">[5] Lasagne</a></li>
  <li><a href="http://www.deeplearning.net/software/theano/">[6] Theano</a></li>
</ul>

<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        this.page.url = 'http://chewning.co';
        this.page.identifier = 'building_cnn_for_nlp';
    };
    */
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = '//chewning.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">
    <div align="center" class="footer-text">
    This site is built from <a href="https://github.com/jekyll/jekyll">jekyll</a>.
    </div>
<!--
  <div class="wrapper">

    <h2 class="footer-heading">chewning.co</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>chewning.co</li>
          <li><a href="mailto:"></a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/init-random"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">init-random</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/keithchewning"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">keithchewning</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>
-->
</footer>


  </body>

</html>
